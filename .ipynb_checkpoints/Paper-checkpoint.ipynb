{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Use \"GloVe: Global Vectors for Word Representation\" to do music recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Also refrenced http://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove/|\n",
    "\n",
    "http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Related methods\n",
    "---\n",
    "### 1.Word Embeddings\n",
    "### Matrix Factorization Methods\n",
    "- LSA\n",
    "\n",
    "The matrices are of “term-document” type.\n",
    "- HAL\n",
    "\n",
    "Utilizes matrices of “term-term” type, a main problem with HAL and related meth- ods is that the most frequent words contribute a disproportionate amount to the similarity measure: the number of times two words co-occur with the or and, for example, will have a large effect on their similarity despite conveying relatively little about their semantic relatedness.\n",
    "- PMI (pointwise mutual information)\n",
    "\n",
    "PMI is a typical measure for the strength of association between two words. It is defined as the log ratio between the joint probability of two words and the product of their marginal probabilities.\n",
    "\n",
    "\n",
    "Levy et al demonstrate that word2vec implicitly factorizes a word-context PMI matrix.\n",
    "\n",
    "### Word2Vec\n",
    "- Skip-gram\n",
    "- CBOW\n",
    "\n",
    "### GloVe\n",
    "\n",
    "The ratio of co-occurence is better able to distinguish relevant words, compared to the raw probabilities.\n",
    "\n",
    "### 2.Recommendations\n",
    "### Collaborative Filtering (CF)\n",
    "CF is a subset of Recommendation algorithms that seeks to exploit users’ interaction as well as their explicit product ratings in order to predict the rating for a user on an unseen product or to predict the propensity of a user to consume an item, for example - buy a product or view a video-content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## How we do it\n",
    "\n",
    "---\n",
    "\n",
    "We treat the songs that a user likes most as analogous to words and the list of songs as sentences. This ‘corpus’ of songs is then used as an input to a word embedding algorithm. The algorithms then learn song vector representations that capture the relationship between songs. The similarity between songs is captured in the representation of the song vectors and is used to build an song similarity matrix. We then use these song-song similarities to generate recommendations for users based on their favourite songs.\n",
    "\n",
    "The similarity between song is computed as a dot product of the song vectors. At the time of prediction, we weigh the song similarity scores with the times the user has listened to those songs in order to rank the set of predicted songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
