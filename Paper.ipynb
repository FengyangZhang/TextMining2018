{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use \"GloVe: Global Vectors for Word Representation\" to do music recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also refrenced http://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove/|\n",
    "\n",
    "http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related methods\n",
    "---\n",
    "### 1.Word Embeddings\n",
    "### Matrix Factorization Methods\n",
    "- LSA\n",
    "\n",
    "The matrices are of “term-document” type.\n",
    "- HAL\n",
    "\n",
    "Utilizes matrices of “term-term” type, a main problem with HAL and related meth- ods is that the most frequent words contribute a disproportionate amount to the similarity measure: the number of times two words co-occur with the or and, for example, will have a large effect on their similarity despite conveying relatively little about their semantic relatedness.\n",
    "- PMI (pointwise mutual information)\n",
    "\n",
    "PMI is a typical measure for the strength of association between two words. It is defined as the log ratio between the joint probability of two words and the product of their marginal probabilities.\n",
    "\n",
    "\n",
    "Levy et al demonstrate that word2vec implicitly factorizes a word-context PMI matrix.\n",
    "\n",
    "### Word2Vec\n",
    "- Skip-gram\n",
    "- CBOW\n",
    "\n",
    "### GloVe\n",
    "\n",
    "The ratio of co-occurence is better able to distinguish relevant words, compared to the raw probabilities.\n",
    "\n",
    "### 2.Recommendations\n",
    "### Collaborative Filtering (CF)\n",
    "CF is a subset of Recommendation algorithms that seeks to exploit users’ interaction as well as their explicit product ratings in order to predict the rating for a user on an unseen product or to predict the propensity of a user to consume an item, for example - buy a product or view a video-content.\n",
    "\n",
    "\n",
    "### Co-occurence based recommendation\n",
    "\n",
    "This approach involves building an item-item matrix where each entry in the matrix reflects the number of times two items co-occur in the user session data. At prediction time, for each item, we predict the top 20 items that co-occur with that item as recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## How we do it\n",
    "\n",
    "---\n",
    "\n",
    "We treat the songs that a user likes most as analogous to words and the list of songs as sentences. This ‘corpus’ of songs is then used as an input to a word embedding algorithm. The algorithms then learn song vector representations that capture the relationship between songs. The similarity between songs is captured in the representation of the song vectors and is used to build an song similarity matrix. We then use these song-song similarities to generate recommendations for users based on their favourite songs.\n",
    "\n",
    "The similarity between song is computed as a dot product of the song vectors. At the time of prediction, we weigh the song similarity scores with the times the user has listened to those songs in order to rank the set of predicted songs. More specifically, after we get the song vectors, we will calculate similarity between each two songs, at test time, for a user, for each song in the song library, we compute the summation of its similarity with the 10 favourite songs of that user. The formula is as follows\n",
    "\n",
    "$$S^*_u = argmax\\sum_{i=1}^{10}sim_{S_{iu}, S}, S\\in C$$\n",
    "\n",
    "Where $S^*_u$ is the recommended song returned by the model for the user, C is all the songs in our dataset. $S_{iu}$ is the ith song that the user $u$ listened to most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
